ggplot(stack(Temp))+geom_boxplot(aes(x=ind,y=values))+labs(x="Site & Depth",y=units[j-1],title=paste(vars[j],' at ',as.character(depths[l]),'m',sep=""))
ggsave(paste(outDir,'/',vars[j],'_',as.character(depths[l]),"_boxplot_ES.png",sep=""),device="png",width=600,height=800,units="px",scale=5,dpi=600)
# Remove outliers?
q25 = quantile(stack(Temp)$values,probs=0.25,na.rm=TRUE)
q75 = quantile(stack(Temp)$values,probs=0.75,na.rm=TRUE)
iqr = q75-q25
upperThresh = q75+(1.5*iqr)
lowerThresh = q25-(1.5*iqr)
# Interpolate missing dates and NAs (can't interpolate JAX at most depths, too little data!)
if (l==1){
for (i in 1:11){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
} else {
for (i in 1:10){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
# add (likely hole-y) JAX data without interpolation
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'=NA',sep="")))
eval(parse(text=paste('putWhere = match(masterData.Time,full',vars[j],'$Time)',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'[putWhere[!is.na(putWhere)]]=Temp[-which(is.na(putWhere)),i+1]',sep="")))
}
}
## Clean HYCOM data --------------------------------
# Downloaded HYCOM data are daily at 2/25th (0.08) degrees irregular spatial resolution, re-gridded to 0.08deg
# vars = c('SSH','Salinity','Temperature','VelocityMag','VelocityAsp','EKE')
vars = 'EKE'
lon = "ES"
for (j in 1:length(vars)){
if (vars[j]=="SSH"){
TSind = which(!is.na(str_match(fileList,vars[j])) & !is.na(str_match(fileList,lon)))
load(paste(inDir,'/',fileList[TSind],sep=""))
sites = c('HZ0','OC0','NC0','BC0','WC0','NFC0','HAT0','GS0','BP0','BS0','JAX0')
SSHDF = data.frame(t(masterData.Data))
colnames(SSHDF) = sites
# Histograms
for (i in 1:11){
eval(parse(text=(paste(sites[i],' = ggplot(data=SSHDF)+geom_histogram(aes(x=',sites[i],'))+labs(x="m",y="",title=sites[i])',sep=""))))
}
png(file=paste(outDir,'/',"SSH_hist.png",sep=""),width = 800, height = 800, units = "px")
grid.arrange(HZ0,OC0,NC0,BC0,WC0,NFC0,HAT0,GS0,BP0,BS0,JAX0, ncol=4,nrow=3,top="SSH")
while (dev.cur()>1) {dev.off()}
# Plot boxplots and calculate quantiles to identify outliers
ggplot(stack(SSHDF))+geom_boxplot(aes(x=ind,y=values))+labs(x="Site & Depth",y="m",title="SSH")
ggsave(paste(outDir,'/',"SSH_boxplot.png",sep=""),device="png",width=600,height=800,units="px",scale=5,dpi=600)
q25 = quantile(stack(SSHDF)$values,probs=0.25)
q75 = quantile(stack(SSHDF)$values,probs=0.75)
iqr = q75-q25
# Remove outliers?
for (i in 1:11){
q = round(SSHDF[,i],digits=10)
SSHDF[q==0,i] = NA
}
# Interpolate missing dates and NAs
fullSSH = data.frame(Time=seq.Date(from=as.Date("2016-02-01",origin="1970-01-01"),to=as.Date("2019-04-30",origin="1970-01-01"),by=1))
for (i in 1:11){
datBins = which(!is.na(SSHDF[,i]))
eval(parse(text=paste('fullSSH$',sites[i],'=NA',sep="")))
eval(parse(text=paste('fullSSH$',sites[i],' = (approx(x=masterData.Time[datBins],y=SSHDF[datBins,i],xout=fullSSH[,1],method="linear"))$y',sep="")))
}
# # Create time lagged vectors
# startInd = which(fullSSH$Time==as.Date('2016-05-01',origin="1970-01-01"))
# fullLength = length(fullSSH$Time)
# for (i in 1:10){
#   for (k in lags){
#     lagInd = startInd-k
#     eval(parse(text=paste('fullSSH$',sites[i],'Lag',as.character(k),' = NA',sep="")))
#     eval(parse(text=paste('fullSSH$',sites[i],'Lag',as.character(k),'[startInd:',fullLength,'] = fullSSH$',sites[i],'[lagInd:(',fullLength,'-',as.character(k),')]',sep="")))
#   }
# }
write.csv(fullSSH,file=paste(inDir,'/','SSH_TS.csv',sep=""),row.names=FALSE)
} else { # Multi-depth Variables
TSind = which(!is.na(str_match(fileList,paste(vars[j],'_Profiles',sep=""))) & !is.na(str_match(fileList,lon)))
load(paste(inDir,'/',fileList[TSind],sep=""))
outDFs = c('fullSalinity','fullTemperature','fullVelocityMag','fullVelocityAsp','fullEKE')
depths = c(0,100,200,300,400,500,600,700,800) # desired depth layers
units = c("PPT",paste(intToUtf8(176),"C",sep=""),"m/s",intToUtf8(176),paste(expression("cm"^2),'/',expression("s"^2),sep=""))
eval(parse(text=paste('full',vars[j],' = data.frame(Time=seq.Date(from=as.Date("2016-02-01",origin="1970-01-01"),to=as.Date("2019-04-30",origin="1970-01-01"),by=1))',sep="")))
for (l in 1:length(depths)){
eval(parse(text=paste('Temp = data.frame(HZ',depths[l],'=HZProfile[l,],
OC',depths[l],'=OCProfile[l,],
NC',depths[l],'=NCProfile[l,],
BC',depths[l],'=BCProfile[l,],
WC',depths[l],'=WCProfile[l,],
NFC',depths[l],'=NFCProfile[l,],
HAT',depths[l],'=HATProfile[l,],
GS',depths[l],'=GSProfile[l,],
BP',depths[l],'=BPProfile[l,],
BS',depths[l],'=BSProfile[l,],
JAX',depths[l],'=JAXProfile[l,])',sep="")))
sites = colnames(Temp)
siteName = str_replace(sites,as.character(depths[l]),"")
for (i in 1:11){
eval(parse(text=paste('Temp$',sites[i],'[is.nan(Temp$',sites[i], ')] = NA',sep="")))
eval(parse(text=paste('Temp$',sites[i],'[Temp$',sites[i],'==0] = NA',sep="")))
}
# Histograms
for (i in 1:11){
eval(parse(text=(paste(siteName[i],' = ggplot(data=Temp)+geom_histogram(aes(x=',
sites[i],'))+labs(x=units[j-1],title=sites[i])',sep=""))))
}
png(file=paste(outDir,'/',vars[j],'_',as.character(depths[l]),"_hist_ES.png",sep=""),width = 800, height = 800, units = "px")
grid.arrange(HZ,NC,OC,BC,WC,NFC,HAT,GS,BP,BS,JAX, ncol=4,nrow=3,top=paste(vars[j],' at ',as.character(depths[l]),'m',sep=""))
while (dev.cur()>1) {dev.off()}
# Plot boxplots and calculate quantiles to identify outliers
ggplot(stack(Temp))+geom_boxplot(aes(x=ind,y=values))+labs(x="Site & Depth",y=units[j-1],title=paste(vars[j],' at ',as.character(depths[l]),'m',sep=""))
ggsave(paste(outDir,'/',vars[j],'_',as.character(depths[l]),"_boxplot_ES.png",sep=""),device="png",width=600,height=800,units="px",scale=5,dpi=600)
# Remove outliers?
q25 = quantile(stack(Temp)$values,probs=0.25,na.rm=TRUE)
q75 = quantile(stack(Temp)$values,probs=0.75,na.rm=TRUE)
iqr = q75-q25
upperThresh = q75+(1.5*iqr)
lowerThresh = q25-(1.5*iqr)
# Interpolate missing dates and NAs (can't interpolate JAX at most depths, too little data!)
if (l==1){
for (i in 1:11){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
} else {
for (i in 1:10){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
# add (likely hole-y) JAX data without interpolation
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'=NA',sep="")))
eval(parse(text=paste('putWhere = match(masterData.Time,full',vars[j],'$Time)',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'[putWhere[!is.na(putWhere)]]=Temp[-which(is.na(putWhere)),i+1]',sep="")))
}
}
if (eval(parse(text=paste('any(full',vars[j],'<0,na.rm=TRUE)',sep="")))){
return('WARNING: SPURIOUS NEGATIVE VALUES CHECK DATA')
}
# # Create time lagged vectors
# eval(parse(text=paste('startInd = which(full',vars[j],'$Time==as.Date("2016-05-01",origin="1970-01-01"))',sep="")))
# eval(parse(text=paste('fullLength = length(full',vars[j],'$Time)',sep="")))
# eval(parse(text=paste('fullColNames = colnames(full',vars[j],')',sep="")))
# for (i in 2:length(fullColNames)){
#   for (k in lags){
#     lagInd = startInd-k
#     eval(parse(text=paste('full',vars[j],'$',fullColNames[i],'Lag',as.character(k),' = NA',sep="")))
#     eval(parse(text=paste('full',vars[j],'$',fullColNames[i],'Lag',as.character(k),'[startInd:',fullLength,'] = full',vars[j],'$',fullColNames[i],'[lagInd:(',fullLength,'-',as.character(k),')]',sep="")))
#   }
# }
# Save data as .csv
eval(parse(text=paste("write.csv(full",vars[j],",file=\"J:/Chpt_3/CovarTS/",vars[j],"_TS.csv\",row.names=FALSE)",sep="")))
}
}
## Clean HYCOM data --------------------------------
# Downloaded HYCOM data are daily at 2/25th (0.08) degrees irregular spatial resolution, re-gridded to 0.08deg
# vars = c('SSH','Salinity','Temperature','VelocityMag','VelocityAsp','EKE')
vars = 'EKE'
lon = "ES"
for (j in 1:length(vars)){
if (vars[j]=="SSH"){
TSind = which(!is.na(str_match(fileList,vars[j])) & !is.na(str_match(fileList,lon)))
load(paste(inDir,'/',fileList[TSind],sep=""))
sites = c('HZ0','OC0','NC0','BC0','WC0','NFC0','HAT0','GS0','BP0','BS0','JAX0')
SSHDF = data.frame(t(masterData.Data))
colnames(SSHDF) = sites
# Histograms
for (i in 1:11){
eval(parse(text=(paste(sites[i],' = ggplot(data=SSHDF)+geom_histogram(aes(x=',sites[i],'))+labs(x="m",y="",title=sites[i])',sep=""))))
}
png(file=paste(outDir,'/',"SSH_hist.png",sep=""),width = 800, height = 800, units = "px")
grid.arrange(HZ0,OC0,NC0,BC0,WC0,NFC0,HAT0,GS0,BP0,BS0,JAX0, ncol=4,nrow=3,top="SSH")
while (dev.cur()>1) {dev.off()}
# Plot boxplots and calculate quantiles to identify outliers
ggplot(stack(SSHDF))+geom_boxplot(aes(x=ind,y=values))+labs(x="Site & Depth",y="m",title="SSH")
ggsave(paste(outDir,'/',"SSH_boxplot.png",sep=""),device="png",width=600,height=800,units="px",scale=5,dpi=600)
q25 = quantile(stack(SSHDF)$values,probs=0.25)
q75 = quantile(stack(SSHDF)$values,probs=0.75)
iqr = q75-q25
# Remove outliers?
for (i in 1:11){
q = round(SSHDF[,i],digits=10)
SSHDF[q==0,i] = NA
}
# Interpolate missing dates and NAs
fullSSH = data.frame(Time=seq.Date(from=as.Date("2016-02-01",origin="1970-01-01"),to=as.Date("2019-04-30",origin="1970-01-01"),by=1))
for (i in 1:11){
datBins = which(!is.na(SSHDF[,i]))
eval(parse(text=paste('fullSSH$',sites[i],'=NA',sep="")))
eval(parse(text=paste('fullSSH$',sites[i],' = (approx(x=masterData.Time[datBins],y=SSHDF[datBins,i],xout=fullSSH[,1],method="linear"))$y',sep="")))
}
# # Create time lagged vectors
# startInd = which(fullSSH$Time==as.Date('2016-05-01',origin="1970-01-01"))
# fullLength = length(fullSSH$Time)
# for (i in 1:10){
#   for (k in lags){
#     lagInd = startInd-k
#     eval(parse(text=paste('fullSSH$',sites[i],'Lag',as.character(k),' = NA',sep="")))
#     eval(parse(text=paste('fullSSH$',sites[i],'Lag',as.character(k),'[startInd:',fullLength,'] = fullSSH$',sites[i],'[lagInd:(',fullLength,'-',as.character(k),')]',sep="")))
#   }
# }
write.csv(fullSSH,file=paste(inDir,'/','SSH_TS.csv',sep=""),row.names=FALSE)
} else { # Multi-depth Variables
TSind = which(!is.na(str_match(fileList,paste(vars[j],'_Profiles',sep=""))) & !is.na(str_match(fileList,lon)))
load(paste(inDir,'/',fileList[TSind],sep=""))
outDFs = c('fullSalinity','fullTemperature','fullVelocityMag','fullVelocityAsp','fullEKE')
depths = c(0,100,200,300,400,500,600,700,800) # desired depth layers
units = c("PPT",paste(intToUtf8(176),"C",sep=""),"m/s",intToUtf8(176),paste(expression("cm"^2),'/',expression("s"^2),sep=""))
eval(parse(text=paste('full',vars[j],' = data.frame(Time=seq.Date(from=as.Date("2016-02-01",origin="1970-01-01"),to=as.Date("2019-04-30",origin="1970-01-01"),by=1))',sep="")))
for (l in 1:length(depths)){
eval(parse(text=paste('Temp = data.frame(HZ',depths[l],'=HZProfile[l,],
OC',depths[l],'=OCProfile[l,],
NC',depths[l],'=NCProfile[l,],
BC',depths[l],'=BCProfile[l,],
WC',depths[l],'=WCProfile[l,],
NFC',depths[l],'=NFCProfile[l,],
HAT',depths[l],'=HATProfile[l,],
GS',depths[l],'=GSProfile[l,],
BP',depths[l],'=BPProfile[l,],
BS',depths[l],'=BSProfile[l,],
JAX',depths[l],'=JAXProfile[l,])',sep="")))
sites = colnames(Temp)
siteName = str_replace(sites,as.character(depths[l]),"")
for (i in 1:11){
eval(parse(text=paste('Temp$',sites[i],'[is.nan(Temp$',sites[i], ')] = NA',sep="")))
eval(parse(text=paste('Temp$',sites[i],'[Temp$',sites[i],'==0] = NA',sep="")))
}
# Histograms
for (i in 1:10){
eval(parse(text=(paste(siteName[i],' = ggplot(data=Temp)+geom_histogram(aes(x=',
sites[i],'))+labs(x=units[j-1],title=sites[i])',sep=""))))
}
png(file=paste(outDir,'/',vars[j],'_',as.character(depths[l]),"_hist_ES.png",sep=""),width = 800, height = 800, units = "px")
grid.arrange(HZ,NC,OC,BC,WC,NFC,HAT,GS,BP,BS,JAX, ncol=4,nrow=3,top=paste(vars[j],' at ',as.character(depths[l]),'m',sep=""))
while (dev.cur()>1) {dev.off()}
# Plot boxplots and calculate quantiles to identify outliers
ggplot(stack(Temp))+geom_boxplot(aes(x=ind,y=values))+labs(x="Site & Depth",y=units[j-1],title=paste(vars[j],' at ',as.character(depths[l]),'m',sep=""))
ggsave(paste(outDir,'/',vars[j],'_',as.character(depths[l]),"_boxplot_ES.png",sep=""),device="png",width=600,height=800,units="px",scale=5,dpi=600)
# Remove outliers?
q25 = quantile(stack(Temp)$values,probs=0.25,na.rm=TRUE)
q75 = quantile(stack(Temp)$values,probs=0.75,na.rm=TRUE)
iqr = q75-q25
upperThresh = q75+(1.5*iqr)
lowerThresh = q25-(1.5*iqr)
# Interpolate missing dates and NAs (can't interpolate JAX at most depths, too little data!)
if (l==1){
for (i in 1:11){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
} else {
for (i in 1:10){
datBins = which(!is.na(Temp[,i]))
eval(parse(text=paste('full',vars[j],'$',sites[i],'=NA',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i],' = (approx(x=masterData.Time[datBins],y=Temp[datBins,i],xout=full',vars[j],'[,1],method="linear"))$y',sep="")))
}
# add (likely hole-y) JAX data without interpolation
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'=NA',sep="")))
eval(parse(text=paste('putWhere = match(masterData.Time,full',vars[j],'$Time)',sep="")))
eval(parse(text=paste('full',vars[j],'$',sites[i+1],'[putWhere[!is.na(putWhere)]]=Temp[-which(is.na(putWhere)),i+1]',sep="")))
}
}
if (eval(parse(text=paste('any(full',vars[j],'<0,na.rm=TRUE)',sep="")))){
return('WARNING: SPURIOUS NEGATIVE VALUES CHECK DATA')
}
# # Create time lagged vectors
# eval(parse(text=paste('startInd = which(full',vars[j],'$Time==as.Date("2016-05-01",origin="1970-01-01"))',sep="")))
# eval(parse(text=paste('fullLength = length(full',vars[j],'$Time)',sep="")))
# eval(parse(text=paste('fullColNames = colnames(full',vars[j],')',sep="")))
# for (i in 2:length(fullColNames)){
#   for (k in lags){
#     lagInd = startInd-k
#     eval(parse(text=paste('full',vars[j],'$',fullColNames[i],'Lag',as.character(k),' = NA',sep="")))
#     eval(parse(text=paste('full',vars[j],'$',fullColNames[i],'Lag',as.character(k),'[startInd:',fullLength,'] = full',vars[j],'$',fullColNames[i],'[lagInd:(',fullLength,'-',as.character(k),')]',sep="")))
#   }
# }
# Save data as .csv
eval(parse(text=paste("write.csv(full",vars[j],",file=\"J:/Chpt_3/CovarTS/",vars[j],"_TS.csv\",row.names=FALSE)",sep="")))
}
}
60^2
debugSource("D:/Code/HabitatModeling/DataWrangling_forHabMods.R", echo=TRUE)
paste('full',vars[j],'$',sites[i+1],'=NA',sep="")
paste('putWhere = match(masterData.Time,full',vars[j],'$Time)',sep="")
paste('full',vars[j],'$',sites[i+1],'[putWhere[!is.na(putWhere)]]=Temp[-which(is.na(putWhere)),i+1]',sep="")
View(fullEKE)
sum(!is.na(putWhere))
load('J:/Chpt_3/CovarTS/EKE_Profiles_ES.Rdata')
View(BCProfile)
View(BCProfile)
plot(BCProfile[1,])
plot(BCProfile[3,])
plot(BCProfile[5,])
plot(BCProfile[7,])
plot(BCProfile[8,])
debugSource("D:/Code/HabitatModeling/DataWrangling_forHabMods.R", echo=TRUE)
library(stringr)
library(dplyr)
library(lubridate)
library(sm)
library(pracma)
library(ggplot2)
library(gridExtra)
library(gridGraphics)
library(rstatix)
library(expss)
library(corrplot)
inDir = 'J:/Chpt_3/ModelData'
outDir = 'J:/Chpt_3/EcologicalNichePlots'
fileList = list.files(path=inDir,pattern=paste('_masterDF.csv',sep=""),
full.names=TRUE,recursive=FALSE,
include.dirs=FALSE,no..=TRUE)
## Compare species covar selections at specific sites ------------
specs = c("UD26","Risso")
sites = c("HZ","OC","NC","BC","WC","NFC","HAT")
covars = c("Chl0","FSLE0","SSH0","Sal0","Temp0","Sal300","Temp300","AEddyDist0","CEddyDist0","GSLat","GSDist")
plotColors = c('#D62A1C','#1C3FD6')
files = c(str_which(fileList,specs[1]),str_which(fileList,specs[2]))
# if it doesn't already exist, create directory to save figures
if (!dir.exists(paste(outDir,'/',specs[1],"_",specs[2],sep=""))){
dir.create(paste(outDir,'/',specs[1],"_",specs[2],sep=""))
}
data1 = data.frame(read.csv(fileList[files[1]]))
data1$Pres = round(data1$Pres)
data2 = data.frame(read.csv(fileList[files[2]]))
data2$Pres = round(data2$Pres)
for (j in 1:length(sites)){
for (i in 1:length(covars)){
# find data at this site
siteInd1 = str_which(data1$Site,sites[j])
siteInd2 = str_which(data2$Site,sites[j])
# Divide covar range into bins
varMin = min(min(data1[[covars[i]]][siteInd1]),min(data2[[covars[i]]][siteInd2]))
varMin = varMin-abs(0.01*varMin)
varMax = max(max(data1[[covars[i]]][siteInd1]),max(data2[[covars[i]]][siteInd2]))
varMax = varMax+abs(0.01*varMax)
varBins = seq(varMin,varMax,length.out=21)
# Identify which bin each covar observation falls into
binDat1 = histc(data1[[covars[i]]][siteInd1],varBins)
binDat2 = histc(data2[[covars[i]]][siteInd2],varBins)
# Calculate average and SD of presence values corresponding to covar obs in each bin
presChar1 = data.frame(pres=data1$Pres[siteInd1],bin=binDat1$bin) %>%
group_by(bin) %>%
summarize(MeanPres=mean(pres,na.rm=TRUE),
SD=sd(pres,na.rm=TRUE))
presChar2 = data.frame(pres=data2$Pres[siteInd2],bin=binDat2$bin) %>%
group_by(bin) %>%
summarize(MeanPres=mean(pres,na.rm=TRUE),
SD=sd(pres,na.rm=TRUE))
# Normalize by max mean presence per species
presChar1$NormMean = presChar1$MeanPres-min(presChar1$MeanPres)
presChar1$NormMean = presChar1$NormMean/max(presChar1$NormMean)
presChar1$NormSD = presChar1$SD-min(presChar1$MeanPres)
presChar1$NormSD = presChar1$NormSD/max(presChar1$NormMean)
presChar2$NormMean = presChar2$MeanPres-min(presChar2$MeanPres)
presChar2$NormMean = presChar2$NormMean/max(presChar2$NormMean)
presChar2$NormSD = presChar2$SD-min(presChar2$MeanPres)
presChar2$NormSD = presChar2$NormSD/max(presChar2$NormMean)
presChar1$binCenter = varBins[presChar1$bin]+0.5*diff(varBins[1:2])
presChar2$binCenter = varBins[presChar2$bin]+0.5*diff(varBins[1:2])
# Plot as overlaid bar charts
barPlot = ggplot(
)+geom_col(data=presChar1,
aes(x=binCenter,y=NormMean),
color=plotColors[1],
fill=plotColors[1],
alpha=0.5
)+geom_col(data=presChar2,
aes(x=binCenter,y=NormMean),
color=plotColors[2],
fill=plotColors[2],
alpha=0.5
)+coord_cartesian(xlim=c(varMin,varMax)
)+labs(x=covars[i],y="Normalized Mean Presence",title=covars[i]
)+theme_minimal()
# make legend
legDF = data.frame(x1=c(1,1),
x2=c(2,2),
y1=c(1,2),
y2=c(1.75,2.75),
cols=plotColors)
legPlot = ggplot(legDF
)+geom_rect(aes(ymin=y1,
ymax=y2,
xmin=x1,
xmax=x2,
color=cols,
fill=cols),
alpha=0.5
)+scale_fill_identity(
)+scale_color_identity(
)+geom_text(aes(x=x2+0.95,
y=y1+0.5,
label=specs),
size=2.75
)+guides(fill='none',color='none'
)+coord_cartesian(xlim=c(1,4)
)+theme_void()
png(file=paste(outDir,'/',specs[1],"_",specs[2],"/",covars[i],"_at_",sites[j],".png",sep=""),width = 500, height = 400, units = "px",res=125)
grid.arrange(barPlot,legPlot,ncol=5,nrow=4,layout_matrix=rbind(c(rep(1,4),NA),
c(rep(1,4),2),
c(rep(1,4),NA),
c(rep(1,4),NA)))
while (dev.cur()>1) {dev.off()}
}
}
## Compare species covar selections at specific sites ------------
specs = c("UD26","Risso")
sites = c("HZ","OC","NC","BC","WC","NFC","HAT")
covars = c("Chl0","FSLE0","SSH0","Sal0","Temp0","Sal400","Temp400","AEddyDist0","CEddyDist0","GSLat","GSDist")
plotColors = c('#D62A1C','#1C3FD6')
files = c(str_which(fileList,specs[1]),str_which(fileList,specs[2]))
# if it doesn't already exist, create directory to save figures
if (!dir.exists(paste(outDir,'/',specs[1],"_",specs[2],sep=""))){
dir.create(paste(outDir,'/',specs[1],"_",specs[2],sep=""))
}
data1 = data.frame(read.csv(fileList[files[1]]))
data1$Pres = round(data1$Pres)
data2 = data.frame(read.csv(fileList[files[2]]))
data2$Pres = round(data2$Pres)
for (j in 1:length(sites)){
for (i in 1:length(covars)){
# find data at this site
siteInd1 = str_which(data1$Site,sites[j])
siteInd2 = str_which(data2$Site,sites[j])
# Divide covar range into bins
varMin = min(min(data1[[covars[i]]][siteInd1]),min(data2[[covars[i]]][siteInd2]))
varMin = varMin-abs(0.01*varMin)
varMax = max(max(data1[[covars[i]]][siteInd1]),max(data2[[covars[i]]][siteInd2]))
varMax = varMax+abs(0.01*varMax)
varBins = seq(varMin,varMax,length.out=21)
# Identify which bin each covar observation falls into
binDat1 = histc(data1[[covars[i]]][siteInd1],varBins)
binDat2 = histc(data2[[covars[i]]][siteInd2],varBins)
# Calculate average and SD of presence values corresponding to covar obs in each bin
presChar1 = data.frame(pres=data1$Pres[siteInd1],bin=binDat1$bin) %>%
group_by(bin) %>%
summarize(MeanPres=mean(pres,na.rm=TRUE),
SD=sd(pres,na.rm=TRUE))
presChar2 = data.frame(pres=data2$Pres[siteInd2],bin=binDat2$bin) %>%
group_by(bin) %>%
summarize(MeanPres=mean(pres,na.rm=TRUE),
SD=sd(pres,na.rm=TRUE))
# Normalize by max mean presence per species
presChar1$NormMean = presChar1$MeanPres-min(presChar1$MeanPres)
presChar1$NormMean = presChar1$NormMean/max(presChar1$NormMean)
presChar1$NormSD = presChar1$SD-min(presChar1$MeanPres)
presChar1$NormSD = presChar1$NormSD/max(presChar1$NormMean)
presChar2$NormMean = presChar2$MeanPres-min(presChar2$MeanPres)
presChar2$NormMean = presChar2$NormMean/max(presChar2$NormMean)
presChar2$NormSD = presChar2$SD-min(presChar2$MeanPres)
presChar2$NormSD = presChar2$NormSD/max(presChar2$NormMean)
presChar1$binCenter = varBins[presChar1$bin]+0.5*diff(varBins[1:2])
presChar2$binCenter = varBins[presChar2$bin]+0.5*diff(varBins[1:2])
# Plot as overlaid bar charts
barPlot = ggplot(
)+geom_col(data=presChar1,
aes(x=binCenter,y=NormMean),
color=plotColors[1],
fill=plotColors[1],
alpha=0.5
)+geom_col(data=presChar2,
aes(x=binCenter,y=NormMean),
color=plotColors[2],
fill=plotColors[2],
alpha=0.5
)+coord_cartesian(xlim=c(varMin,varMax)
)+labs(x=covars[i],y="Normalized Mean Presence",title=covars[i]
)+theme_minimal()
# make legend
legDF = data.frame(x1=c(1,1),
x2=c(2,2),
y1=c(1,2),
y2=c(1.75,2.75),
cols=plotColors)
legPlot = ggplot(legDF
)+geom_rect(aes(ymin=y1,
ymax=y2,
xmin=x1,
xmax=x2,
color=cols,
fill=cols),
alpha=0.5
)+scale_fill_identity(
)+scale_color_identity(
)+geom_text(aes(x=x2+0.95,
y=y1+0.5,
label=specs),
size=2.75
)+guides(fill='none',color='none'
)+coord_cartesian(xlim=c(1,4)
)+theme_void()
png(file=paste(outDir,'/',specs[1],"_",specs[2],"/",covars[i],"_at_",sites[j],".png",sep=""),width = 500, height = 400, units = "px",res=125)
grid.arrange(barPlot,legPlot,ncol=5,nrow=4,layout_matrix=rbind(c(rep(1,4),NA),
c(rep(1,4),2),
c(rep(1,4),NA),
c(rep(1,4),NA)))
while (dev.cur()>1) {dev.off()}
}
}
setwd("J:/Chpt_3/GAM_Output/True")
load("J:/Chpt_3/GAM_Output/True/WeeklyRegionalModel.Rdata")
View(weekModCompTable)
load("J:/Chpt_3/GAM_Output/SpermWhale/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/Sowerby/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/SFPW/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/SBCD/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/Risso/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/Kogia/WeeklyRegionalModel.Rdata")
load("J:/Chpt_3/GAM_Output/Gervais/WeeklyRegionalModel.Rdata")
setwd("D:/Code/Temporal_Analyses")
